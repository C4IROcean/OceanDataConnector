{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2994db-5af7-48c7-bc60-23a90d78d64d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification\n",
    "This notebook will explore the data from the dataset that was created in `CRW_bleaching.ipynb`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741605e-8754-485c-a570-5e92369f1566",
   "metadata": {},
   "source": [
    "## Useful libraries\n",
    "In this notebook we use the following python libraries. If you are interested you can find more infromation about them in the links below:\n",
    "- [Pandas](https://pandas.pydata.org/docs/user_guide/index.html#user-guide): Used for data manipulation and analysis\n",
    "- [Matplotlib](https://matplotlib.org/stable/tutorials/index.html): Used to visualize data\n",
    "- [Numpy](https://numpy.org/doc/stable/user/whatisnumpy.html): Used for mathematical operations on vectors and matrices.\n",
    "- [Seaborn](https://seaborn.pydata.org/): Used to visualize statistical data\n",
    "- [Scikit learn](https://scikit-learn.org/stable/user_guide.html): Used to pre-process and classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743b429-a514-4b6a-ac42-7d9f25d6612b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd789a-0f61-4884-baea-fa10c2235740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b06633-363e-4e6c-86fc-d99559976e68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the table with information from the coral reefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a39ad-f789-4031-a73f-9bf026a715e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_reef_data_df = pd.read_csv(\"bleaching_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2006d-666e-4864-b4d6-65cdea95f1db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inspect \n",
    "To get a better overview of the data we want to use for our classification it is useful to look into the distribution of the data in question. A nice way of doing this is thorugh a histogram plot showing how the datapoints are distributed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c8dd7-a6fa-4124-a294-a34982a2f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that visualizes the distribution of data points for a list of columns\n",
    "def plot_multiple_histograms(columns, dataframe):\n",
    "    num_cols = len(columns)\n",
    "    inspect_df = dataframe[columns]\n",
    "    fig, ax = plt.subplots(1, num_cols, sharex=\"col\", sharey=\"row\", figsize=(30, 3))\n",
    "    for i in range(num_cols):\n",
    "        inspect_df.hist(column=inspect_df.columns[i], bins=20, ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033bbed-5a84-4439-af83-f24845af28b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect data that releates to bleaching\n",
    "In our dataset we have two features that indicate the bleaching level of a reef these are:<br>\n",
    "`Bleaching_Level` and `Percent_Bleached`<br>\n",
    "Percent bleached indicates the observed bleaching level of the coral in % while Bleaching level is levels of bleaching ranging from 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c20291-f0a4-4ba7-ad3d-f42762fb9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the columns that relate to bleaching data and plot to get distribution of data\n",
    "bleaching_cols = [\"Bleaching_Level\", \"Percent_Bleached\"]\n",
    "\n",
    "plot_multiple_histograms(columns=bleaching_cols, dataframe=coral_reef_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc9fef-0835-4223-9df3-b5952f6f419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df that only cotains the two features\n",
    "bleaching_df = coral_reef_data_df[bleaching_cols]\n",
    "# Look at how the `Bleaching_Level` relates to `Percent_Bleached`\n",
    "bleaching_df_not_na = bleaching_df[\n",
    "    (bleaching_df[\"Bleaching_Level\"].notna())\n",
    "    & (bleaching_df[\"Percent_Bleached\"].notna())\n",
    "]\n",
    "bleaching_df_not_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133aaeaa-4caf-4c46-a72b-e57e69530f83",
   "metadata": {},
   "source": [
    "Take a look at the distribution of the data in the frame<br>\n",
    "What does this tell us about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleaching_df_not_na.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f54b73-325f-441a-86d4-a1ba76d4ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in each column\n",
    "bleaching_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657cf8f-a59b-4470-87fd-a35a2257e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many null values do we have for each column?\n",
    "bleaching_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd330b-f99c-4b13-bb81-707250fb3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleaching_df[bleaching_df.Bleaching_Level == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784d1ca-943d-4729-886d-7ebf789aa123",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "for row in bleaching_df_not_na.iterrows():\n",
    "    if row[1].Bleaching_Level == 1:\n",
    "        c1.append(row[1].Percent_Bleached)\n",
    "    elif row[1].Bleaching_Level == 2:\n",
    "        c2.append(row[1].Percent_Bleached)\n",
    "    elif row[1].Bleaching_Level == 3:\n",
    "        c3.append(row[1].Percent_Bleached)\n",
    "\n",
    "print(f\"Bleaching level \\t Number of entries \\t Min \\t Max\")\n",
    "print(f\"1 \\t\\t\\t {len(c1)} \\t\\t\\t {np.min(c1)} \\t {np.max(c1)}\")\n",
    "print(f\"2 \\t\\t\\t {len(c2)} \\t\\t\\t {np.min(c2)} \\t {np.max(c2)}\")\n",
    "print(f\"3 \\t\\t\\t {len(c3)} \\t\\t\\t {np.min(c3)} \\t {np.max(c3)}\")\n",
    "print(f\"total \\t\\t\\t {len(c1)+len(c2)+len(c3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d9e1a-7093-449c-99f0-b8a663e17004",
   "metadata": {},
   "source": [
    "We can observe that there are different measures of bleaching. Some rows only have entries for `Bleaching_Level`, while other rows only have entries for `Percent_Bleached`. Let us create a new column where we try to join the information from these two categories. \n",
    "\n",
    "Another observation is that we have a -1 value for the bleaching level. Unfortunately, the creators of the database did not explain what `-1` means in this context, so we will remove it to be safe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39d53a-787e-454e-8a59-eeecb9e2dabd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create a column in the table with categorical bleaching data based on information in `Bleaching_Level` and `Percent_Bleached`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bf1c3-44d7-4d6d-a25b-aa27e407a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the `Bleaching_Level` is -1\n",
    "coral_reef_data_df = coral_reef_data_df.where(\n",
    "    coral_reef_data_df[\"Bleaching_Level\"] != -1\n",
    ")\n",
    "\n",
    "# Create a copy of the column `Bleaching_Level` and call it `Categorical_Percent_Bleached`\n",
    "coral_reef_data_df[\"Categorical_Percent_Bleached\"] = coral_reef_data_df[\n",
    "    \"Bleaching_Level\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301b401-4a5e-4171-90bf-4605e700e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where `Categorical_Percent_Bleached` is not set, and ´Percent_Bleached´ is between 0 and 15 and set the `Categorical_Percent_Bleached` to 1\n",
    "coral_reef_data_df.loc[\n",
    "    (\n",
    "        (coral_reef_data_df[\"Categorical_Percent_Bleached\"].isna())\n",
    "        & (coral_reef_data_df[\"Percent_Bleached\"] >= 0)\n",
    "        & (coral_reef_data_df[\"Percent_Bleached\"] < 15)\n",
    "    ),\n",
    "    \"Categorical_Percent_Bleached\",\n",
    "] = 1\n",
    "\n",
    "# Find rows where `Categorical_Percent_Bleached` is not set, and ´Percent_Bleached´ is between 15 and 50 and set the `Categorical_Percent_Bleached` to 2\n",
    "coral_reef_data_df.loc[\n",
    "    (\n",
    "        (coral_reef_data_df[\"Categorical_Percent_Bleached\"].isna())\n",
    "        & (coral_reef_data_df[\"Percent_Bleached\"] >= 15)\n",
    "        & (coral_reef_data_df[\"Percent_Bleached\"] < 50)\n",
    "    ),\n",
    "    \"Categorical_Percent_Bleached\",\n",
    "] = 2\n",
    "\n",
    "# Find rows where `Categorical_Percent_Bleached` is not set, and ´Percent_Bleached´ is above 50 and set the `Categorical_Percent_Bleached` to 3\n",
    "coral_reef_data_df.loc[\n",
    "    (\n",
    "        (coral_reef_data_df[\"Categorical_Percent_Bleached\"].isna())\n",
    "        & (coral_reef_data_df[\"Percent_Bleached\"] >= 50)\n",
    "    ),\n",
    "    \"Categorical_Percent_Bleached\",\n",
    "] = 3\n",
    "\n",
    "# Drop rows where `Categorical_Percent_Bleached` is not set\n",
    "coral_reef_data_df = coral_reef_data_df.dropna(subset=[\"Categorical_Percent_Bleached\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fabffd-51d4-4eca-9be7-b1ecbfe407ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_df = coral_reef_data_df[[\"Categorical_Percent_Bleached\"]]\n",
    "\n",
    "inspect_df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036279ee-cdb7-4aa9-88c3-9d90b88e63f6",
   "metadata": {},
   "source": [
    "We now have a column with one consistent measurement for coral bleaching. Lets move on to our feature evaluation to see what parameters affect coral bleaching.\n",
    "\n",
    "In the next section we will look into our temperature and thermal stress data and see what effects these features have on coral bleaching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4013d-1d4c-4d77-8712-3e83f630cfb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect Temperature and Wind data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba93124-9346-4890-a5f8-d96baa8c5394",
   "metadata": {},
   "source": [
    "The temperature data is based on weekly Sea Surface Temperatures ([SSTs](https://climatedataguide.ucar.edu/climate-data/sst-data-sets-overview-comparison-table)) for the study time frame, [created using a harmonics approach](https://journals.ametsoc.org/view/journals/atot/30/7/jtech-d-12-00195_1.xml). Windspeed is also included in this data.\n",
    "\n",
    "\n",
    "<details><summary>Open to view information about each Temperature and wind parameter</summary>\n",
    "\n",
    "|Parameter|Description|\n",
    "|---|---|\n",
    "|**Temperature_Kelvin**|SST in Kelvin.|\n",
    "|**Temperature_Mean**|Mean SST in degrees Celsius.|\n",
    "|**Temperature_Minimum**|Minimum SST in degrees Celsius.|\n",
    "|**Temperature_Maximum**|Maximum SST in degrees Celsius.|\n",
    "|**Temperature_Kelvin_Standard_Deviation**|Standard deviation of SST in Kelvin.|\n",
    "|**Windspeed**|Meters per hour.|\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808c4f3-e7f6-43f5-9453-38a4716e4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Temperature data\n",
    "temp_cols = [\"Temperature_Kelvin\"]\n",
    "all_temp_cols = [\n",
    "    \"Temperature_Kelvin\",\n",
    "    \"Temperature_Mean\",\n",
    "    \"Temperature_Minimum\",\n",
    "    \"Temperature_Maximum\",\n",
    "]\n",
    "std_cols = [\"Temperature_Kelvin_Standard_Deviation\"]\n",
    "wind_cols = [\"Windspeed\"]\n",
    "all_cols = temp_cols + std_cols + wind_cols\n",
    "plot_multiple_histograms(columns=all_cols, dataframe=coral_reef_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a108f60-a074-4545-b9e9-7b0d20bb1312",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "If you want to, you can try to convert all temprature related columns presented in Kelvin to Celsius [formula](https://climatedataguide.ucar.edu/climate-data/sst-data-sets-overview-comparison-table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42803d-80a3-4aff-9db7-347872f0065d",
   "metadata": {},
   "source": [
    "#### Data distribution box plot\n",
    "A boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). It can tell you about your outliers and what their values are. Read more about the method [here](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256387f2-7e93-4d65-9e1d-190012ac6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.boxplot(data=coral_reef_data_df[temp_cols])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc57107-dbcc-4306-a94f-a78a44ef98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.boxplot(data=coral_reef_data_df[wind_cols])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbec903-50e8-4e20-ab0c-0332bedf3f23",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "Should we remove some of our outliers from the table? if so remove them, if not keep them and continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9299c22-61bc-4e92-829d-f1b2ef5060c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect Sea-Surface Temperature Anomaly (SSTA) data\n",
    "Some sea surface temperature anomalies are simply transient events, not part of a specific pattern or trend. Other anomalies are more meaningful. At irregular intervals (roughly every 3-6 years), the sea surface temperatures in the Pacific Ocean along the equator become warmer or cooler than normal. These anomalies are the hallmark of El Niño and La Niña climate cycles, which can influence weather patterns across the globe.\n",
    "\n",
    "Strong, localized sea surface temperature anomalies may reveal that an ocean current, such as the Gulf Stream Current off the east coast of the United States, has veered off its usual path for a time or is stronger or weaker than usual. Sea surface temperature anomalies that persist over many years can be signals of regional or global climate change, such as global warming. The warm anomaly appears to intensify in the Northern Hemisphere summer. This pattern results from the fact that sea ice is retreating to a smaller area in the summer now than in the past; areas that used to be covered with ice all summer are now open water.\n",
    "\n",
    "Sea surface temperature anomalies have practical as well as scientific applications. For example, in coastal areas, anomalous temperatures (either warm or cool) can favor one organism in an ecosystem over another, causing populations of one kind of bacteria, algae, or fish to thrive or decline. Warm sea surface temperature anomalies can also warn natural resource managers where coral reefs may be in danger of bleaching. \n",
    "[Source](https://earthobservatory.nasa.gov/global-maps/AMSRE_SSTAn_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc009f-e108-4793-854e-716fc26f9d90",
   "metadata": {},
   "source": [
    "The Sea-Surface Temperature Anomaly (SSTA) is the weekly SST minus weekly climatological SST in degrees Celsius.\n",
    "\n",
    "SSTA_DHW (Sea-Surface Temperature Degree Heating Weeks) is the sum of the previous 12 weeks when SSTA >  = 1 degree Celsius.\n",
    "\n",
    "<details><summary>Open to view information about each SSTA parameter</summary>\n",
    "\n",
    "|Parameter|Description|\n",
    "|---|---|\n",
    "|**SSTA**|(Sea-Surface Temperature Anomaly) weekly SST minus weekly climatological SST.|\n",
    "|**SSTA_Standard_Deviation**|The Standard Deviation of weekly SSTA in degrees Celsius over the entire period.|\n",
    "|**SSTA_Mean**|The mean SSTA in degrees Celsius over the entire period.|\n",
    "|**SSTA_Minimum**|The minimum SSTA in degrees Celsius over the entire period.|\n",
    "|**SSTA_Maximum**|The maximum SSTA in degrees Celsius over the entire period.|\n",
    "|**SSTA_Frequency**|(Sea Surface Temperature Anomaly Frequency) number of times over the previous 52 weeks that SSTA >  = 1 degree Celsius.|\n",
    "|**SSTA_Frequency_Standard_Deviation**|The standard deviation of SSTA Frequency in degrees Celsius over the entire time period of 40 years.|\n",
    "|**SSTA_FrequencyMax**|The maximum SSTA Frequency in degrees Celsius over the entire time period.|\n",
    "|**SSTA_FrequencyMean**|The mean SSTA Frequency in degrees Celsius over the entire time period of 40 years.|\n",
    "|-|-|\n",
    "|**SSTA_DHW**|(Sea Surface Temperature Degree Heating Weeks) sum of previous 12 weeks when SSTA > = 1 degree.|\n",
    "|**SSTA_DHW_Standard_Deviation**|The standard deviation SSTA DHW in degrees Celsius over the entire period.|\n",
    "|**SSTA_DHWMax**|The maximum SSTA DHW in degrees Celsius over the entire time period of 40 years.|\n",
    "|**SSTA_DHWMean**|The mean SSTA DHW in degrees Celsius over the entire time period of 40 years.|\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e64292-87db-4d5c-93d4-a7de56bd673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect SSTA data\n",
    "ssta_cols = [\n",
    "    \"SSTA\",\n",
    "    \"SSTA_Standard_Deviation\",\n",
    "    \"SSTA_Mean\",\n",
    "    \"SSTA_Minimum\",\n",
    "    \"SSTA_Maximum\",\n",
    "    \"SSTA_Frequency\",\n",
    "    \"SSTA_Frequency_Standard_Deviation\",\n",
    "    \"SSTA_FrequencyMax\",\n",
    "    \"SSTA_FrequencyMean\",\n",
    "    \"SSTA_DHW\",\n",
    "    \"SSTA_DHW_Standard_Deviation\",\n",
    "    \"SSTA_DHWMax\",\n",
    "    \"SSTA_DHWMean\",\n",
    "]\n",
    "ssta_measurment_cols = [\"SSTA\", \"SSTA_Frequency\", \"SSTA_DHW\"]\n",
    "plot_multiple_histograms(columns=ssta_measurment_cols, dataframe=coral_reef_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efcfaf-6f5e-4ae7-aafe-b5a550916099",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "ax = sns.boxplot(data=coral_reef_data_df[ssta_measurment_cols])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a837737-66d4-4085-a1ea-2a8bd766104b",
   "metadata": {},
   "source": [
    "Should we remove some of our outliers from the table? if so remove them, if not keep them and continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bff694-8838-42a0-9605-cb9a12ba756e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect Thermal Stress Anomaly (TSA) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1731576-3e4d-48cb-8cb0-74782d8c1f8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Thermal stress disrupts the symbiotic relationship between the corals and the algae living in their tissues, which causes coral bleaching. [Read more here](https://oceanservice.noaa.gov/facts/coral_bleach.html#:~:text=When%20corals%20are%20stressed%20by,them%20to%20turn%20completely%20white.&text=Warmer%20water%20temperatures%20can%20result%20in%20coral%20bleaching.)\n",
    "\n",
    "Thermal Stress Anomalies are defined as deviations of 1 week where the temperature was 1 degree Celsius or greater than the mean maximum climatological week(= the long term average warmest week from 1985 to 2005).  \n",
    "\n",
    "`TSA_DHW` (Thermal Stress Anomaly - Degree Heating Week) is the sum of the previous 12 weeks when TSA >= 1 degree Celsius. \n",
    "\n",
    "<details><summary>Open to view information about each TSA parameter</summary>\n",
    "\n",
    "|Parameter|Description|\n",
    "|---|---|\n",
    "|**TSA_Standard_Deviation**|The standard deviation of TSA in degrees Celsius over the entire time period of 40 years.|\n",
    "|**TSA_Minimum**|The minimum TSA in degrees Celsius over the entire time period of 40 years.|  \n",
    "|**TSA_Maximum**|The maximum TSA in degrees Celsius over the entire time period of 40 years.|\n",
    "|**TSA_Mean**|The mean TSA in degrees Celsius over the entire time period of 40 years.|\n",
    "|**TSA_Frequency**|The number of times over previous 52 weeks that TSA >= 1 degree Celsius.|  \n",
    "|**TSA_Frequency_Standard_Deviation**|The standard deviation of frequency of TSA in degrees Celsius over the entire time period of 40 years.|  \n",
    "|**TSA_FrequencyMax**|The maximum TSA frequency in degrees Celsius over the entire time period of 40 years.|  \n",
    "|**TSA_FrequencyMean**|The mean TSA frequency in degrees Celsius over the entire time period of 40 years.|  \n",
    "|-|-|\n",
    "|**TSA_DHW**|Sum of previous 12 weeks when TSA >= 1 degree Celsius. |\n",
    "|**TSA_DHW_Standard_Deviation**|The standard deviation of TSA DHW in degrees Celsius over the entire time period of 40 years.|\n",
    "|**TSA_DHWMax**|The maximum TSA DHW in degrees Celsius over the entire time period of 40 years.|\n",
    "|**TSA_DHWMean**|The mean TSA DHW in degrees Celsius over the entire time period of 40 years|\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c7b9d-7aae-4a19-8ed2-5557f001f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsa_cols = [\n",
    "    \"TSA_Mean\",\n",
    "    \"TSA_Frequency\",\n",
    "    \"TSA_Frequency_Standard_Deviation\",\n",
    "    \"TSA_FrequencyMax\",\n",
    "    \"TSA_FrequencyMean\",\n",
    "    \"TSA_DHW\",\n",
    "    \"TSA_DHW_Standard_Deviation\",\n",
    "    \"TSA_DHWMax\",\n",
    "    \"TSA_DHWMean\",\n",
    "]\n",
    "tsa_measurment_cols = [\"TSA_Mean\", \"TSA_DHW\"]\n",
    "plot_multiple_histograms(columns=tsa_cols, dataframe=coral_reef_data_df)\n",
    "# choose columns based on available data\n",
    "tsa_cols = [\n",
    "    \"TSA_Mean\",\n",
    "    \"TSA_Frequency\",\n",
    "    \"TSA_Frequency_Standard_Deviation\",\n",
    "    \"TSA_FrequencyMax\",\n",
    "    \"TSA_FrequencyMean\",\n",
    "    \"TSA_DHW\",\n",
    "    \"TSA_DHW_Standard_Deviation\",\n",
    "    \"TSA_DHWMax\",\n",
    "    \"TSA_DHWMean\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cfa90-8ee8-4566-b627-541a60026615",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "ax = sns.boxplot(data=coral_reef_data_df[tsa_cols])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a578db9-41f0-465b-9b0f-f941e34ef6b5",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "What can we say about these features by looking at their distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67810139-55e4-4fd0-bc48-004a1c94a87f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot correlation\n",
    "Correlation is used to find the relationship between two variables, this is often used to predict the value of one variable with the help of other variables, who is being correlated with it.\n",
    "A good way to quickly check correlations among our selected columns is by visualizing the correlation matrix as a heatmap. The stronger the color, the larger the correlation magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6fb74-1e5b-4018-a0e9-7a3a0a1607b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_reef_correlation = coral_reef_data_df[\n",
    "    [\"Categorical_Percent_Bleached\"]\n",
    "    + temp_cols\n",
    "    + wind_cols\n",
    "    + tsa_measurment_cols\n",
    "    + ssta_measurment_cols\n",
    "].corr()\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(coral_reef_correlation, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation between variables variables and bleaching\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9980e-9b29-4ef3-bed8-4c6cc9f25754",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select the most important features\n",
    "\n",
    "### Challenge\n",
    "Based on the matrix above, which features would you like to select for our classification model? \n",
    "Create a dataset based on the selected columns that has values for categorical percent bleached\n",
    "\n",
    "#### Some suggestions to get you started:\n",
    "- Feel fre to try out other feature selections methods, a [comprehensive guide can be found here](kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection/notebook) \n",
    "- You can also try [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a10f3-87a8-4ded-8fad-7692b697f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try other feature selection methos here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793126bf-9a54-4abc-8d46-5009b6af8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that don't have a value for `Categorical_Percent_Bleached`\n",
    "coral_reef_data_df = coral_reef_data_df.dropna(subset=\"Categorical_Percent_Bleached\")\n",
    "# Select features that can be used to classify. Are there any features that you wish to remove from our data?\n",
    "chosen_columns = [\n",
    "    \"TSA_Mean\",\n",
    "    \"TSA_Frequency\",\n",
    "    \"TSA_Frequency_Standard_Deviation\",\n",
    "    \"TSA_FrequencyMax\",\n",
    "    \"TSA_FrequencyMean\",\n",
    "    \"TSA_DHW\",\n",
    "    \"TSA_DHW_Standard_Deviation\",\n",
    "    \"TSA_DHWMax\",\n",
    "    \"TSA_DHWMean\",\n",
    "    \"Temperature_Kelvin\",\n",
    "    \"Temperature_Mean\",\n",
    "    \"Temperature_Minimum\",\n",
    "    \"Temperature_Maximum\",\n",
    "    \"Temperature_Kelvin_Standard_Deviation\",\n",
    "    \"Windspeed\",\n",
    "    \"SSTA\",\n",
    "    \"SSTA_Standard_Deviation\",\n",
    "    \"SSTA_Mean\",\n",
    "    \"SSTA_Minimum\",\n",
    "    \"SSTA_Maximum\",\n",
    "    \"SSTA_Frequency\",\n",
    "    \"SSTA_Frequency_Standard_Deviation\",\n",
    "    \"SSTA_FrequencyMax\",\n",
    "    \"SSTA_FrequencyMean\",\n",
    "    \"SSTA_DHW\",\n",
    "    \"SSTA_DHW_Standard_Deviation\",\n",
    "    \"SSTA_DHWMax\",\n",
    "    \"SSTA_DHWMean\",\n",
    "]\n",
    "\n",
    "features_df = coral_reef_data_df[chosen_columns]\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857af563",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Normalization\n",
    "Normalization is a technique often applied as part of data preparation for machine learning. \n",
    "The goal of normalization is to change the values of numeric columns in the dataset to a common scale, \n",
    "without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. \n",
    "It is required only when features have different ranges.\n",
    "\n",
    "### Challenge \n",
    "Do you have to nomalize the data? If yes, perform the normalization on the selected columns. If no continue on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97f082-8f5f-402e-a78d-eb7fdd496642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ce449-f9c4-4fe9-8f25-1123200f9c7d",
   "metadata": {},
   "source": [
    "## Building our model\n",
    "When our model is ready it can take data as input and output if an area is bleached or not. Thus it is important that we create a full data set when traning the model, all our selected feature data will be given as `X`, and our truth `Categorical_Percent_Bleached` will be denoted as `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebfd73e-9cfa-46ea-909c-b62ea219bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = coral_reef_data_df[chosen_columns + [\"Categorical_Percent_Bleached\"]]\n",
    "full_df = full_df.dropna()\n",
    "X_df = full_df[chosen_columns]\n",
    "# Replace all NaN values with 0  ( should we drop nan instead?)\n",
    "X = X_df.to_numpy()\n",
    "X.shape\n",
    "\n",
    "# Normalize the data w standard scaling\n",
    "from sklearn import preprocessing\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_norm = std_scale.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658f090-d9fc-46a3-b39c-f64907dfb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_column = [\"Categorical_Percent_Bleached\"]\n",
    "y_df = full_df[y_column]\n",
    "# Flatten the numpy array.\n",
    "y = y_df.to_numpy().ravel()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c413047-3af9-4ced-a326-487c16b1b30c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c862d6c-ecfc-4785-9741-60926df65c17",
   "metadata": {},
   "source": [
    "When using a supervised algorithm to classify data, it is necessary to provide the classifier with the correct class for each row in the data used for training the classifier. We therefore need to split our data into a training- and a test-dataset. The training data will be used by the classifier optimize it's result. The test-data will be used to evaluate the classifier on data it has not seen before. \n",
    "\n",
    "Below we have chosen a classifier called Linear Discriminant Analysis. More information about this classifier can be found [here](https://scikit-learn.org/stable/modules/lda_qda.html#estimation-algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332d744-fd76-44b3-a862-7f1b5eee1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556f5eb-544f-41fa-87b9-1f6cde26f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test. When we set the `test_size` to 0.2, 80% of the data will be used in the training phase, and 20% will be saved for the test phase.\n",
    "X = np.nan_to_num(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddbd40-3dbd-43bd-b91c-69169b7aec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us plot a histogram plot that lets us visualize the number of entries for each category in our dataset, train-dataset and test-dataset.\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "fig.suptitle(\"Distribution of classes in our dataset\")\n",
    "# y\n",
    "_, _, bars = axes[0].hist(y, bins=3)\n",
    "axes[0].set_xticks([1, 2, 3])\n",
    "axes[0].set_title(\"y\")\n",
    "axes[0].bar_label(bars)\n",
    "# y_train\n",
    "_, _, bars = axes[1].hist(y_train, bins=3)\n",
    "axes[1].set_xticks([1, 2, 3])\n",
    "axes[1].set_title(\"y_train\")\n",
    "axes[1].bar_label(bars)\n",
    "# y_test\n",
    "_, _, bars = axes[2].hist(y_test, bins=3)\n",
    "axes[2].set_xticks([1, 2, 3])\n",
    "axes[2].set_title(\"y_test\")\n",
    "axes[2].bar_label(bars)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1472a-1add-4835-8800-50ecc833c92b",
   "metadata": {},
   "source": [
    "We clearly have a very imbalanced dataset. The problem with an unbalanced dataset is that one category will be better represented during training. The model can learn to always predict one category, because it will get a high accuracy by predicting the same class over and over. There are a few ways we can work around this. \n",
    "- One option is to resample our dataset. [More information can be found in this article.](https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb)\n",
    "    - Undersampling: We can choose fewer entries from the categories where we have a lot of data.\n",
    "    - Oversampling: We can make copies of the entries in the categories where we have less data. \n",
    "- Another option is to use a metric that shows us where the model errs. \n",
    "    - The confusion matrix plot illustrates clearly what the model predicts compared to the true label. A model that is not confused has high values on the diagonal, and low elsewhere. \n",
    "    - A precision-recall curve shows the tradeoff between precision and recall. A high area under the curve is good, because it represents both high recall and high precision. \n",
    "\n",
    "Try to balance the datsets. What are the advantages and disadvatages of the methods? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9f6cd-8a18-45c8-8392-1b63e8f40c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(f\"accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a268b3b-2ad5-408c-9664-f4f47684d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(clf, X_test, y_test, cmap=\"YlOrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32610b6-5ce8-4dfc-a152-5bfd810e36d7",
   "metadata": {},
   "source": [
    "To find a good classifier, it can be necessary to try a few more, and to try to set the parameter of the classifier to something different. Below is a function that tests several classifiers. More information about the classifiers can be found [here](https://scikit-learn.org/stable/modules/multiclass.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab68e7c-fabd-497a-830b-6d1c4a36e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC  # (setting multi_class=”crammer_singer”)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def evaluate_classifier(clf, name):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    print(f\"{name} \\t accuracy = {accuracy}\")\n",
    "    return accuracy, name\n",
    "\n",
    "\n",
    "def evaluate_classifiers(classifiers, names):\n",
    "    best_accuracy = 0\n",
    "    best_accuracy_clf = None\n",
    "    for clf, name in zip(classifiers, names):\n",
    "        accuracy, clf_name = evaluate_classifier(clf, name)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_accuracy_clf = clf_name\n",
    "    print(f\"\\n\\nBEST ACCURACY = {best_accuracy} \\t {best_accuracy_clf}\")\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    BernoulliNB(),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    KNeighborsClassifier(n_neighbors=3),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "]\n",
    "names = [\n",
    "    \"LDA\",\n",
    "    \"BernoulliNB\",\n",
    "    \"GaussianNB\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"QDA\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"SVC\",\n",
    "]\n",
    "\n",
    "evaluate_classifiers(classifiers, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2d424-875c-43cc-ad01-53c79d369c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_for_multiple_classifiers(classifiers, names):\n",
    "    fig, axes = plt.subplots(1, len(classifiers), figsize=(30, 5))\n",
    "    for i in range(len(classifiers)):\n",
    "        clf = classifiers[i]\n",
    "        name = names[i]\n",
    "        clf.fit(X_train, y_train)\n",
    "        plot_confusion_matrix(clf, X_test, y_test, ax=axes[i], cmap=\"YlOrRd\")\n",
    "        axes[i].set_title(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_confusion_matrix_for_multiple_classifiers(classifiers, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee46bcc-f197-404e-abca-30e916bc7654",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Precision-Recall\n",
    "\n",
    "Precision-Recall is a measure of the accuracy and relevance of the results. \n",
    "Precision is defined as $P = \\frac{T_p}{T_p + F_p}$, where $T_p$ is the number of true positives and $F_p$ is the number of false positives. \n",
    "Recall is defined as $R = \\frac{T_p}{T_p + F_n}$, where $F_n$ is the number of false negatives. \n",
    "\n",
    "A more thorough guide on Precision and Recall can be found [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#:~:text=The%20precision%2Drecall%20curve%20shows,a%20low%20false%20negative%20rate.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75778c44-c1dd-4213-b676-40b18c6bf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b5a39-08a7-4b13-a31b-133823ff6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use label_binarize to be multi-label like settings\n",
    "Y = label_binarize(y, classes=[1, 2, 3])\n",
    "n_classes = Y.shape[1]\n",
    "\n",
    "# Split into training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "# create a classification pipeline\n",
    "classifier = OneVsRestClassifier(\n",
    "    make_pipeline(StandardScaler(), QuadraticDiscriminantAnalysis())\n",
    ")\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_score = classifier.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c941620-3814-4df5-a34d-8dd7344389f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "    Y_test.ravel(), y_score.ravel()\n",
    ")\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test, y_score, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61750c5c-d840-4148-9e09-0d41e684408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# setup plot details\n",
    "colors = cycle([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\n",
    "\n",
    "_, ax = plt.subplots(figsize=(7, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines, labels = [], []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "    plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "display = PrecisionRecallDisplay(\n",
    "    recall=recall[\"micro\"],\n",
    "    precision=precision[\"micro\"],\n",
    "    average_precision=average_precision[\"micro\"],\n",
    ")\n",
    "display.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[i],\n",
    "        precision=precision[i],\n",
    "        average_precision=average_precision[i],\n",
    "    )\n",
    "    display.plot(ax=ax, name=f\"Precision-recall for class {i}\", color=color)\n",
    "\n",
    "# add the legend for the iso-f1 curves\n",
    "handles, labels = display.ax_.get_legend_handles_labels()\n",
    "handles.extend([l])\n",
    "labels.extend([\"iso-f1 curves\"])\n",
    "# set the legend and the axes\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "ax.set_title(\"Extension of Precision-Recall curve to multi-class\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9461d2-0bdb-4b41-8694-36e4adaaffd6",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "What can you conclude about the precision and recall of this model?<br>\n",
    "In this specific use-case, which do you think is more important to optimize for, precision or recall? If you have time you can tune the model to get better precision or recall based on your decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368a8a9-24e8-40b3-bebe-2013525f1617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
